{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55cd5e50-fce3-496d-a85d-cd3d709d9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnxruntime-gpu in /opt/conda/lib/python3.12/site-packages (1.22.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (24.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (5.28.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime-gpu) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.12/site-packages (from opencv-python) (2.0.2)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime-gpu\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cd46885-a24e-48cb-a634-c62cdc85127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2193f066-1f81-4988-8978-959b48c82a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "available_providers = ort.get_available_providers()\n",
    "print(available_providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d54b12f7-6f4a-4d8d-8979-994d16c0b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ort.InferenceSession('model/end2end.onnx', providers=[\"CUDAExecutionProvider\"])\n",
    "model = ort.InferenceSession('model/Posenet-Mobilenet.onnx', providers=[\"CUDAExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16001f84-040b-4387-bf72-1e67524c9e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeArg(name='image', type='tensor(float)', shape=[1, 3, 513, 257])\n"
     ]
    }
   ],
   "source": [
    "input_names = [input.name for input in model.get_inputs()]\n",
    "output_names = [output.name for output in model.get_outputs()]\n",
    "\n",
    "input_details = model.get_inputs()\n",
    "print(input_details[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e52ce86-20ec-4fac-b753-fad5fbeb9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmpose_data_preprocessor(image: np.ndarray) -> np.ndarray:\n",
    "    image = cv2.resize(image, (257, 513))\n",
    "    mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n",
    "    std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # rgb_image = rgb_image.astype(np.float32)\n",
    "\n",
    "    normalized_image = (rgb_image - mean.reshape(1, 1, 3)) / std.reshape(1, 1, 3)\n",
    "\n",
    "    chw_image = normalized_image.transpose((2, 0, 1))\n",
    "\n",
    "    return chw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae057824-e748-4fe9-89d6-5a6314784970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('dance.mkv') # Replace with your video file path\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: # Break the loop if no more frames are available\n",
    "        break\n",
    "    # print(frame.shape)\n",
    "    pp_img = mmpose_data_preprocessor(frame)\n",
    "    pp_img = np.expand_dims(pp_img, axis=0)\n",
    "    # print(pp_img.shape)\n",
    "    start = time.perf_counter()\n",
    "    output = model.run(output_names, {input_names[0]: pp_img})\n",
    "    end = time.perf_counter()\n",
    "    if frame_count == 6*30:\n",
    "        break\n",
    "    # print(len(output))\n",
    "    # for o in output:\n",
    "    #     print(o.shape)\n",
    "    # print(end-start)\n",
    "    \n",
    "\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "print(frame_count)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040789cc-d0ba-436f-8a86-e623a8bc1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import posenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a04151-cd29-4dad-9f5d-1bba7c95a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--model', type=int, default=101)\n",
    "parser.add_argument('--cam_id', type=int, default=0)\n",
    "parser.add_argument('--cam_width', type=int, default=1280)\n",
    "parser.add_argument('--cam_height', type=int, default=720)\n",
    "parser.add_argument('--scale_factor', type=float, default=0.7125)\n",
    "\n",
    "\n",
    "while True:\n",
    "    input_image, display_image, output_scale = posenet.read_cap(\n",
    "        cap, scale_factor=0.7125, output_stride=output_stride)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_image = torch.Tensor(input_image).cuda()\n",
    "\n",
    "        heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = model(input_image)\n",
    "\n",
    "        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n",
    "            heatmaps_result.squeeze(0),\n",
    "            offsets_result.squeeze(0),\n",
    "            displacement_fwd_result.squeeze(0),\n",
    "            displacement_bwd_result.squeeze(0),\n",
    "            output_stride=output_stride,\n",
    "            max_pose_detections=10,\n",
    "            min_pose_score=0.15)\n",
    "\n",
    "    keypoint_coords *= output_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de58134c-b1b2-4854-934c-c0f925d875b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Detected Keypoints on Original Image (Confidence >= 0.2) ---\n",
      "nose           : X=527  , Y=765  , Confidence=0.4783\n",
      "leftEye        : X=502  , Y=786  , Confidence=0.3494\n",
      "rightEye       : X=541  , Y=825  , Confidence=0.3767\n",
      "leftEar        : X=696  , Y=818  , Confidence=0.2714\n",
      "rightEar       : X=464  , Y=782  , Confidence=0.4402\n",
      "leftShoulder   : X=579  , Y=982  , Confidence=0.7767\n",
      "rightShoulder  : X=379  , Y=1038 , Confidence=0.3226\n",
      "leftElbow      : X=633  , Y=1079 , Confidence=0.3119\n",
      "rightElbow     : X=476  , Y=1079 , Confidence=0.1440\n",
      "leftWrist      : X=671  , Y=1079 , Confidence=0.3802\n",
      "rightWrist     : X=749  , Y=1079 , Confidence=0.1498\n",
      "leftHip        : X=892  , Y=1079 , Confidence=0.0937\n",
      "rightHip       : X=661  , Y=1079 , Confidence=0.2133\n",
      "leftKnee       : X=721  , Y=1079 , Confidence=0.0655\n",
      "rightKnee      : X=729  , Y=1079 , Confidence=0.1018\n",
      "leftAnkle      : X=956  , Y=1079 , Confidence=0.0111\n",
      "rightAnkle     : X=297  , Y=1079 , Confidence=0.0148\n",
      "\n",
      "--- Detected Keypoints on Original Image (Confidence >= 0.9) ---\n",
      "nose           : Not detected (or below threshold)\n",
      "leftEye        : Not detected (or below threshold)\n",
      "rightEye       : Not detected (or below threshold)\n",
      "leftEar        : Not detected (or below threshold)\n",
      "rightEar       : Not detected (or below threshold)\n",
      "leftShoulder   : Not detected (or below threshold)\n",
      "rightShoulder  : Not detected (or below threshold)\n",
      "leftElbow      : Not detected (or below threshold)\n",
      "rightElbow     : Not detected (or below threshold)\n",
      "leftWrist      : Not detected (or below threshold)\n",
      "rightWrist     : Not detected (or below threshold)\n",
      "leftHip        : Not detected (or below threshold)\n",
      "rightHip       : Not detected (or below threshold)\n",
      "leftKnee       : Not detected (or below threshold)\n",
      "rightKnee      : Not detected (or below threshold)\n",
      "leftAnkle      : Not detected (or below threshold)\n",
      "rightAnkle     : Not detected (or below threshold)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standard PoseNet keypoint mapping (by index)\n",
    "KEYPOINT_NAMES = [\n",
    "    \"nose\", \"leftEye\", \"rightEye\", \"leftEar\", \"rightEar\",\n",
    "    \"leftShoulder\", \"rightShoulder\", \"leftElbow\", \"rightElbow\",\n",
    "    \"leftWrist\", \"rightWrist\", \"leftHip\", \"rightHip\",\n",
    "    \"leftKnee\", \"rightKnee\", \"leftAnkle\", \"rightAnkle\"\n",
    "]\n",
    "\n",
    "def decode_single_person_pose(\n",
    "    heatmaps_tensor: np.ndarray,  # Shape: (1, 17, H, W)\n",
    "    offsets_tensor: np.ndarray,   # Shape: (1, 34, H, W)\n",
    "    output_stride: int,\n",
    "    original_image_shape: tuple,  # (original_height, original_width)\n",
    "    model_input_size: tuple,      # (height, width) image was resized to for model input\n",
    "    confidence_threshold: float = 0.2\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Decodes keypoint coordinates for a single person from raw PoseNet heatmaps\n",
    "    and offset tensors, scaling them to the original image dimensions.\n",
    "\n",
    "    This function is primarily for single-person scenarios or to get the\n",
    "    strongest keypoint detections overall. It does NOT perform multi-person\n",
    "    decoding (i.e., it won't group keypoints for multiple individuals).\n",
    "\n",
    "    Args:\n",
    "        heatmaps_tensor (np.ndarray): The heatmap tensor (1, 17, H, W) in NumPy format.\n",
    "        offsets_tensor (np.ndarray): The offset tensor (1, 34, H, W) in NumPy format.\n",
    "        output_stride (int): The stride of the PoseNet model (e.g., 16 or 32).\n",
    "        original_image_shape (tuple): (height, width) of the image before any resizing.\n",
    "        model_input_size (tuple): (height, width) of the image *after* resizing for the model input.\n",
    "                                 e.g., (257, 257) or (513, 513).\n",
    "        confidence_threshold (float): Minimum confidence for a keypoint to be returned.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are keypoint names (e.g., 'nose', 'leftWrist')\n",
    "              and values are tuples (x, y, confidence) in original image coordinates.\n",
    "              Keypoints not meeting the confidence threshold will be `None`.\n",
    "    \"\"\"\n",
    "    if heatmaps_tensor.ndim != 4 or offsets_tensor.ndim != 4:\n",
    "        raise ValueError(\"Input tensors must be 4-dimensional (batch, channels, H, W).\")\n",
    "    if heatmaps_tensor.shape[0] != 1 or offsets_tensor.shape[0] != 1:\n",
    "        raise ValueError(\"This function expects a batch size of 1 for simplicity.\")\n",
    "    if heatmaps_tensor.shape[1] != len(KEYPOINT_NAMES): # Channels dimension\n",
    "        raise ValueError(f\"Heatmap channels ({heatmaps_tensor.shape[1]}) must match number of keypoints ({len(KEYPOINT_NAMES)}).\")\n",
    "    if offsets_tensor.shape[1] != len(KEYPOINT_NAMES) * 2: # Channels dimension\n",
    "        raise ValueError(f\"Offset channels ({offsets_tensor.shape[1]}) must match 2 * number of keypoints ({len(KEYPOINT_NAMES)*2}).\")\n",
    "\n",
    "    # Extract relevant parts, removing batch dimension\n",
    "    # Now, channels come first: (17, H, W) for heatmaps, (34, H, W) for offsets\n",
    "    heatmaps = heatmaps_tensor.squeeze(0)\n",
    "    offsets = offsets_tensor.squeeze(0)\n",
    "\n",
    "    # Get spatial dimensions from heatmaps (channels, H, W)\n",
    "    num_keypoints_hm, heatmap_height, heatmap_width = heatmaps.shape\n",
    "\n",
    "    detected_keypoints = {}\n",
    "    \n",
    "    # Calculate scaling factors to map from model input coordinates to original image coordinates\n",
    "    original_h, original_w = original_image_shape\n",
    "    model_input_h, model_input_w = model_input_size\n",
    "    \n",
    "    scale_x = original_w / model_input_w\n",
    "    scale_y = original_h / model_input_h\n",
    "\n",
    "    for kp_idx, kp_name in enumerate(KEYPOINT_NAMES):\n",
    "        # Select the heatmap for the current keypoint: (H, W)\n",
    "        keypoint_heatmap = heatmaps[kp_idx, :, :]\n",
    "\n",
    "        # Find the max confidence score and its location (y, x) in the heatmap\n",
    "        max_score = np.max(keypoint_heatmap)\n",
    "\n",
    "        if max_score < confidence_threshold:\n",
    "            # Keypoint not confident enough\n",
    "            detected_keypoints[kp_name] = None\n",
    "            continue\n",
    "\n",
    "        # Get the (y, x) indices of the maximum score\n",
    "        max_score_indices = np.unravel_index(np.argmax(keypoint_heatmap), keypoint_heatmap.shape)\n",
    "        coarse_y, coarse_x = max_score_indices\n",
    "\n",
    "        # Retrieve the corresponding offsets\n",
    "        # Offsets channels are typically ordered as [x0, y0, x1, y1, ...] for each keypoint\n",
    "        # So, for kp_idx, x_offset_channel = kp_idx * 2, y_offset_channel = kp_idx * 2 + 1\n",
    "        offset_x = offsets[kp_idx * 2, coarse_y, coarse_x]\n",
    "        offset_y = offsets[kp_idx * 2 + 1, coarse_y, coarse_x]\n",
    "\n",
    "        # Calculate raw (unscaled) keypoint coordinates in the model's input coordinate system\n",
    "        # These are usually float values\n",
    "        raw_x = coarse_x * output_stride + offset_x\n",
    "        raw_y = coarse_y * output_stride + offset_y\n",
    "\n",
    "        # Scale to original image dimensions and convert to integer pixels\n",
    "        final_x = int(round(raw_x * scale_x))\n",
    "        final_y = int(round(raw_y * scale_y))\n",
    "\n",
    "        # Clamp coordinates to be within original image bounds (0 to width-1, 0 to height-1)\n",
    "        final_x = max(0, min(final_x, original_w - 1))\n",
    "        final_y = max(0, min(final_y, original_h - 1))\n",
    "\n",
    "        detected_keypoints[kp_name] = (final_x, final_y, max_score)\n",
    "\n",
    "    return detected_keypoints\n",
    "\n",
    "# --- Example Usage (Simulating Output Tensors) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define example parameters (you'll need to know these from your model's pre-processing)\n",
    "    ORIGINAL_IMAGE_SHAPE = (1080, 1920) # (height, width) of your actual input image\n",
    "    MODEL_INPUT_SIZE = (257, 513)      # (height, width) the image was resized/padded to for model input\n",
    "                                       # Derived from 33 * 16 and 17 * 16 if output_stride = 16\n",
    "    OUTPUT_STRIDE = 16                 # Common for MobileNet-PoseNet\n",
    "\n",
    "    # Calculate heatmap/offset map dimensions based on model input size and output stride\n",
    "    # H = model_input_height / output_stride, W = model_input_width / output_stride\n",
    "    HM_H = MODEL_INPUT_SIZE[0] // OUTPUT_STRIDE # 528 // 16 = 33\n",
    "    HM_W = MODEL_INPUT_SIZE[1] // OUTPUT_STRIDE # 272 // 16 = 17\n",
    "\n",
    "\n",
    "    # --- Run the decoding function ---\n",
    "    keypoints_on_original_image = decode_single_person_pose(\n",
    "        heatmaps_tensor=output[0],\n",
    "        offsets_tensor=output[1],\n",
    "        output_stride=OUTPUT_STRIDE,\n",
    "        original_image_shape=ORIGINAL_IMAGE_SHAPE,\n",
    "        model_input_size=MODEL_INPUT_SIZE,\n",
    "        confidence_threshold=0.00001 # Default threshold\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Detected Keypoints on Original Image (Confidence >= 0.2) ---\")\n",
    "    for kp_name, coords in keypoints_on_original_image.items():\n",
    "        if coords:\n",
    "            x, y, confidence = coords\n",
    "            print(f\"{kp_name:15}: X={x:<5}, Y={y:<5}, Confidence={confidence:.4f}\")\n",
    "        else:\n",
    "            print(f\"{kp_name:15}: Not detected (or below threshold)\")\n",
    "\n",
    "    # --- Test with a higher confidence threshold ---\n",
    "    print(\"\\n--- Detected Keypoints on Original Image (Confidence >= 0.9) ---\")\n",
    "    keypoints_high_confidence = decode_single_person_pose(\n",
    "        heatmaps_tensor=output[0],\n",
    "        offsets_tensor=output[1],\n",
    "        output_stride=OUTPUT_STRIDE,\n",
    "        original_image_shape=ORIGINAL_IMAGE_SHAPE,\n",
    "        model_input_size=MODEL_INPUT_SIZE,\n",
    "        confidence_threshold=0.9\n",
    "    )\n",
    "    for kp_name, coords in keypoints_high_confidence.items():\n",
    "        if coords:\n",
    "            x, y, confidence = coords\n",
    "            print(f\"{kp_name:15}: X={x:<5}, Y={y:<5}, Confidence={confidence:.4f}\")\n",
    "        else:\n",
    "            print(f\"{kp_name:15}: Not detected (or below threshold)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa91f22-6630-4e97-8948-f4041cb54b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d733fb48-6223-4c66-b16b-b2c2e1d71df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34, 33, 17)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
